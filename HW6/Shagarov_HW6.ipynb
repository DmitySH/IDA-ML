{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 6: классификация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом домашнем задании вам предстоит построить классификатор текстов!\n",
    "\n",
    "Данные мы будем использовать из Kaggle соревнования: https://www.kaggle.com/competitions/nlp-getting-started/data Оттуда надо скачать файл train.csv. На обучающую и тестовую выборки его поделим кодом ниже, менять его не надо!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем работать с датасетом постов из твиттера. Нам предстоит решать задачу бинарной классификации - определять содержатся ли в твитте информация о настоящей катастрофе/инциденте или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "    id keyword                       location  \\\n0    1     NaN                            NaN   \n1    4     NaN                            NaN   \n2    5     NaN                            NaN   \n3    6     NaN                            NaN   \n4    7     NaN                            NaN   \n5    8     NaN                            NaN   \n6   10     NaN                            NaN   \n7   13     NaN                            NaN   \n8   14     NaN                            NaN   \n9   15     NaN                            NaN   \n10  16     NaN                            NaN   \n11  17     NaN                            NaN   \n12  18     NaN                            NaN   \n13  19     NaN                            NaN   \n14  20     NaN                            NaN   \n15  23     NaN                            NaN   \n16  24     NaN                            NaN   \n17  25     NaN                            NaN   \n18  26     NaN                            NaN   \n19  28     NaN                            NaN   \n20  31     NaN                            NaN   \n21  32     NaN                            NaN   \n22  33     NaN                            NaN   \n23  34     NaN                            NaN   \n24  36     NaN                            NaN   \n25  37     NaN                            NaN   \n26  38     NaN                            NaN   \n27  39     NaN                            NaN   \n28  40     NaN                            NaN   \n29  41     NaN                            NaN   \n30  44     NaN                            NaN   \n31  48  ablaze                     Birmingham   \n32  49  ablaze  Est. September 2012 - Bristol   \n33  50  ablaze                         AFRICA   \n34  52  ablaze               Philadelphia, PA   \n35  53  ablaze                     London, UK   \n36  54  ablaze                       Pretoria   \n37  55  ablaze                   World Wide!!   \n38  56  ablaze                            NaN   \n39  57  ablaze                 Paranaque City   \n40  59  ablaze                 Live On Webcam   \n41  61  ablaze                            NaN   \n42  62  ablaze                      milky way   \n43  63  ablaze                            NaN   \n44  64  ablaze                            NaN   \n45  65  ablaze                            NaN   \n46  66  ablaze      GREENSBORO,NORTH CAROLINA   \n47  67  ablaze                            NaN   \n48  68  ablaze                 Live On Webcam   \n49  71  ablaze                       England.   \n\n                                                 text  target  \n0   Our Deeds are the Reason of this #earthquake M...       1  \n1              Forest fire near La Ronge Sask. Canada       1  \n2   All residents asked to 'shelter in place' are ...       1  \n3   13,000 people receive #wildfires evacuation or...       1  \n4   Just got sent this photo from Ruby #Alaska as ...       1  \n5   #RockyFire Update => California Hwy. 20 closed...       1  \n6   #flood #disaster Heavy rain causes flash flood...       1  \n7   I'm on top of the hill and I can see a fire in...       1  \n8   There's an emergency evacuation happening now ...       1  \n9   I'm afraid that the tornado is coming to our a...       1  \n10        Three people died from the heat wave so far       1  \n11  Haha South Tampa is getting flooded hah- WAIT ...       1  \n12  #raining #flooding #Florida #TampaBay #Tampa 1...       1  \n13            #Flood in Bago Myanmar #We arrived Bago       1  \n14  Damage to school bus on 80 in multi car crash ...       1  \n15                                     What's up man?       0  \n16                                      I love fruits       0  \n17                                   Summer is lovely       0  \n18                                  My car is so fast       0  \n19                       What a goooooooaaaaaal!!!!!!       0  \n20                             this is ridiculous....       0  \n21                                  London is cool ;)       0  \n22                                        Love skiing       0  \n23                              What a wonderful day!       0  \n24                                           LOOOOOOL       0  \n25                     No way...I can't eat that shit       0  \n26                              Was in NYC last week!       0  \n27                                 Love my girlfriend       0  \n28                                          Cooool :)       0  \n29                                 Do you like pasta?       0  \n30                                           The end!       0  \n31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n32  We always try to bring the heavy. #metal #RT h...       0  \n33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n34                 Crying out for more! Set me ablaze       0  \n35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n36  @PhDSquares #mufc they've built so much hype a...       0  \n37  INEC Office in Abia Set Ablaze - http://t.co/3...       1  \n38  Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...       1  \n39                             Ablaze for you Lord :D       0  \n40  Check these out: http://t.co/rOI2NSmEJJ http:/...       0  \n41  on the outside you're ablaze and alive\\nbut yo...       0  \n42  Had an awesome time visiting the CFC head offi...       0  \n43       SOOOO PUMPED FOR ABLAZE ???? @southridgelife       0  \n44  I wanted to set Chicago ablaze with my preachi...       0  \n45  I gained 3 followers in the last week. You? Kn...       0  \n46  How the West was burned: Thousands of wildfire...       1  \n47  Building the perfect tracklist to life leave t...       0  \n48  Check these out: http://t.co/rOI2NSmEJJ http:/...       0  \n49  First night with retainers in. It's quite weir...       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>8</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#flood #disaster Heavy rain causes flash flood...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>13</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm on top of the hill and I can see a fire in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>14</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>There's an emergency evacuation happening now ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I'm afraid that the tornado is coming to our a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Three people died from the heat wave so far</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>19</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>20</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Damage to school bus on 80 in multi car crash ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>23</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What's up man?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>24</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>I love fruits</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Summer is lovely</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>My car is so fast</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>28</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a goooooooaaaaaal!!!!!!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>31</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>this is ridiculous....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>32</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>London is cool ;)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>33</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Love skiing</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>34</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>What a wonderful day!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>36</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>LOOOOOOL</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>37</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>No way...I can't eat that shit</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>38</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Was in NYC last week!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>39</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Love my girlfriend</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>40</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Cooool :)</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>41</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Do you like pasta?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>44</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>The end!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>48</td>\n      <td>ablaze</td>\n      <td>Birmingham</td>\n      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>49</td>\n      <td>ablaze</td>\n      <td>Est. September 2012 - Bristol</td>\n      <td>We always try to bring the heavy. #metal #RT h...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>50</td>\n      <td>ablaze</td>\n      <td>AFRICA</td>\n      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>52</td>\n      <td>ablaze</td>\n      <td>Philadelphia, PA</td>\n      <td>Crying out for more! Set me ablaze</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>53</td>\n      <td>ablaze</td>\n      <td>London, UK</td>\n      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>54</td>\n      <td>ablaze</td>\n      <td>Pretoria</td>\n      <td>@PhDSquares #mufc they've built so much hype a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>55</td>\n      <td>ablaze</td>\n      <td>World Wide!!</td>\n      <td>INEC Office in Abia Set Ablaze - http://t.co/3...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>56</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>57</td>\n      <td>ablaze</td>\n      <td>Paranaque City</td>\n      <td>Ablaze for you Lord :D</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>59</td>\n      <td>ablaze</td>\n      <td>Live On Webcam</td>\n      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>61</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>on the outside you're ablaze and alive\\nbut yo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>62</td>\n      <td>ablaze</td>\n      <td>milky way</td>\n      <td>Had an awesome time visiting the CFC head offi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>63</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>SOOOO PUMPED FOR ABLAZE ???? @southridgelife</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>64</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>I wanted to set Chicago ablaze with my preachi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>65</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>I gained 3 followers in the last week. You? Kn...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>66</td>\n      <td>ablaze</td>\n      <td>GREENSBORO,NORTH CAROLINA</td>\n      <td>How the West was burned: Thousands of wildfire...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>67</td>\n      <td>ablaze</td>\n      <td>NaN</td>\n      <td>Building the perfect tracklist to life leave t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>68</td>\n      <td>ablaze</td>\n      <td>Live On Webcam</td>\n      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>71</td>\n      <td>ablaze</td>\n      <td>England.</td>\n      <td>First night with retainers in. It's quite weir...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1 (0.5 балла)\n",
    "\n",
    "Выведете на экран информацию о пропусках в данных. Если пропуски присутствуют заполните их пустой строкой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "id             0\nkeyword       44\nlocation    1760\ntext           0\ntarget         0\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Число пустых значений по столбцам.\n",
    "train.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "id            0\nkeyword      17\nlocation    773\ntext          0\ntarget        0\ndtype: int64"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Число пустых значений по столбцам.\n",
    "test.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "         id                keyword                      location  \\\n1186   1707      bridge%20collapse                                 \n4071   5789                   hail        Carol Stream, Illinois   \n5461   7789                 police                      Houston    \n5787   8257                rioting                                 \n7445  10656                 wounds                Lake Highlands   \n151     216    airplane%20accident           Somewhere Out There   \n915    1324                 bloody  Isolated City In World Perth   \n1305   1886                burning                                 \n2570   3685                destroy       he/him or she/her (ask)   \n7399  10587                wounded   Maracay y Nirgua, Venezuela   \n7474  10691                  wreck    ?currently writing a book?   \n5012   7148               mudslide       Malibu/SantaFe/Winning!   \n1398   2017             casualties                Canadian bread   \n183     262              ambulance                     Amsterdam   \n3113   4468           electrocuted                                 \n2908   4176                  drown                                 \n7147  10243                volcano          West Coast, Cali USA   \n471     681                 attack                        Mumbai   \n967    1399             body%20bag                      New York   \n209     296            annihilated                 Higher Places   \n1961   2825                cyclone                     hyderabad   \n896    1297                 bloody                            65   \n6443   9218      suicide%20bombing         Principality of Zeron   \n2328   3346             demolished                                 \n4645   6604              inundated                                 \n1773   2546              collision              Denver, Colorado   \n3872   5505                 flames          Somewhere Around You   \n2299   3299               demolish                          NYHC   \n1209   1741    buildings%20burning           GO BLUE! HAIL YES!!   \n4054   5759         forest%20fires                 Nicola Valley   \n7134  10218                volcano                                 \n5901   8429              sandstorm                                 \n1251   1807  buildings%20on%20fire                            UK   \n5243   7497            oil%20spill                  New York, NY   \n4047   5748         forest%20fires                                 \n2881   4141                drought                         miami   \n4581   6514               injuries    Madison, WI & St. Louis MO   \n4686   6662              landslide                 Austin, Texas   \n292     427             apocalypse                   Oregon, USA   \n2698   3870             detonation                                 \n3834   5455     first%20responders                                 \n5003   7136               military                                 \n2963   4256               drowning                      Coventry   \n272     396             apocalypse                      ColoRADo   \n2042   2932                 danger          Hailing from Dayton    \n3937   5598                  flood                      New York   \n7590  10844                                                        \n2691   3860             detonation                                 \n3473   4968              explosion        London, United Kingdom   \n1086   1570                   bomb                                 \n\n                                                   text  target  \n1186  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0  \n4071  GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...       1  \n5461  CNN: Tennessee movie theater shooting suspect ...       1  \n5787  Still rioting in a couple of hours left until ...       1  \n7445  Crack in the path where I wiped out this morni...       0  \n151   Experts in France begin examining airplane deb...       1  \n915   'I came to kill Indians...for FUN': Video of s...       1  \n1305  @JohnsonTionne except idk them?? it's really b...       0  \n2570                                  destroy the house       0  \n7399  Police Officer Wounded Suspect Dead After Exch...       1  \n7474  I'm a friggin wreck destiel sucks (read the vi...       0  \n5012  STERLING-SCOTT on the Red Carpet at a fundrais...       1  \n1398  @LibertarianLuke I'm all for that to be honest...       0  \n183   http://t.co/7xGLah10zL Twelve feared killed in...       1  \n3113  got electrocuted last night at work for the fi...       0  \n2908  Some older Native Australians believe that the...       0  \n7147  The Architect Behind Kanye WestÛªs Volcano ht...       0  \n471   India shud not give any evidence 2 pak.They wi...       1  \n967   AUTH LOUIS VUITTON BROWN SAUMUR 35 CROSS BODY ...       0  \n209   The episode where Trunks annihilated Freiza is...       0  \n1961  @roughdeal1 ante hudhud cyclone Chandrababu Va...       1  \n896   @zhenghxn i tried 11 eyes akame ga kill and to...       0  \n6443  @RayquazaErk There are Christian terrorists to...       1  \n2328  @JackMulholland1 I think also became THE MARQU...       0  \n4645  Let people surf!  HI is no Waimea bay. It's no...       0  \n1773  Motorcyclist bicyclist injured in Denver colli...       1  \n3872  Maryland mansion fire that killed 6 caused by ...       1  \n2299  If you think going to demolish Drake's house o...       1  \n1209  @1acd4900c1424d1 @FoxNews no one is rioting bu...       1  \n4054  Forest fires &amp; dying salmon: time 2 act no...       1  \n7134  http://t.co/Ns1AgGFNxz #shoes Asics GT-II Supe...       0  \n5901  Watch This Airport Get Swallowed Up By A Sands...       1  \n1251  #TweetLikeItsSeptember11th2001 Those two build...       1  \n5243  California oil spill might be larger than proj...       1  \n4047  Cartoon Bears. Without them we would qave no k...       0  \n2881  @_gaabyx we got purple activist I thought it w...       1  \n4581  @BuffoonMike I knew mo not doing much would bi...       0  \n4686  @toddstarnes Enjoy the impending landslide Tod...       0  \n292   GO LOOK AT GRIZZLY PEAK RIGHT NOW... It looks ...       0  \n2698  Ignition Knock (Detonation) Sensor-Senso Stand...       0  \n3834  This week first responders and DART members ar...       1  \n5003  Lot of 20 Tom Clancy Military Mystery Novels -...       0  \n2963  Why are you drowning in low self-image? Take t...       0  \n272      I'm gonna fight Taylor as soon as I get there.       0  \n2042  I wish I could get Victoria's Secret on front....       0  \n3937  Spot Flood Combo 53inch 300W Curved Cree LED W...       0  \n7590  SEVERE WEATHER BULLETIN No. 5 FOR: TYPHOON ÛÏ...       1  \n2691  Ignition Knock (Detonation) Sensor-Senso fits ...       1  \n3473  Around 10 injured in explosion in chemical par...       1  \n1086                Soul food sound so bomb right now '       0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1186</th>\n      <td>1707</td>\n      <td>bridge%20collapse</td>\n      <td></td>\n      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4071</th>\n      <td>5789</td>\n      <td>hail</td>\n      <td>Carol Stream, Illinois</td>\n      <td>GREAT MICHIGAN TECHNIQUE CAMP\\nB1G THANKS TO @...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5461</th>\n      <td>7789</td>\n      <td>police</td>\n      <td>Houston</td>\n      <td>CNN: Tennessee movie theater shooting suspect ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5787</th>\n      <td>8257</td>\n      <td>rioting</td>\n      <td></td>\n      <td>Still rioting in a couple of hours left until ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7445</th>\n      <td>10656</td>\n      <td>wounds</td>\n      <td>Lake Highlands</td>\n      <td>Crack in the path where I wiped out this morni...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>216</td>\n      <td>airplane%20accident</td>\n      <td>Somewhere Out There</td>\n      <td>Experts in France begin examining airplane deb...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>1324</td>\n      <td>bloody</td>\n      <td>Isolated City In World Perth</td>\n      <td>'I came to kill Indians...for FUN': Video of s...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1305</th>\n      <td>1886</td>\n      <td>burning</td>\n      <td></td>\n      <td>@JohnsonTionne except idk them?? it's really b...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2570</th>\n      <td>3685</td>\n      <td>destroy</td>\n      <td>he/him or she/her (ask)</td>\n      <td>destroy the house</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7399</th>\n      <td>10587</td>\n      <td>wounded</td>\n      <td>Maracay y Nirgua, Venezuela</td>\n      <td>Police Officer Wounded Suspect Dead After Exch...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7474</th>\n      <td>10691</td>\n      <td>wreck</td>\n      <td>?currently writing a book?</td>\n      <td>I'm a friggin wreck destiel sucks (read the vi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5012</th>\n      <td>7148</td>\n      <td>mudslide</td>\n      <td>Malibu/SantaFe/Winning!</td>\n      <td>STERLING-SCOTT on the Red Carpet at a fundrais...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1398</th>\n      <td>2017</td>\n      <td>casualties</td>\n      <td>Canadian bread</td>\n      <td>@LibertarianLuke I'm all for that to be honest...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>262</td>\n      <td>ambulance</td>\n      <td>Amsterdam</td>\n      <td>http://t.co/7xGLah10zL Twelve feared killed in...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3113</th>\n      <td>4468</td>\n      <td>electrocuted</td>\n      <td></td>\n      <td>got electrocuted last night at work for the fi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2908</th>\n      <td>4176</td>\n      <td>drown</td>\n      <td></td>\n      <td>Some older Native Australians believe that the...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7147</th>\n      <td>10243</td>\n      <td>volcano</td>\n      <td>West Coast, Cali USA</td>\n      <td>The Architect Behind Kanye WestÛªs Volcano ht...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>471</th>\n      <td>681</td>\n      <td>attack</td>\n      <td>Mumbai</td>\n      <td>India shud not give any evidence 2 pak.They wi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>967</th>\n      <td>1399</td>\n      <td>body%20bag</td>\n      <td>New York</td>\n      <td>AUTH LOUIS VUITTON BROWN SAUMUR 35 CROSS BODY ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>209</th>\n      <td>296</td>\n      <td>annihilated</td>\n      <td>Higher Places</td>\n      <td>The episode where Trunks annihilated Freiza is...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1961</th>\n      <td>2825</td>\n      <td>cyclone</td>\n      <td>hyderabad</td>\n      <td>@roughdeal1 ante hudhud cyclone Chandrababu Va...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>1297</td>\n      <td>bloody</td>\n      <td>65</td>\n      <td>@zhenghxn i tried 11 eyes akame ga kill and to...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6443</th>\n      <td>9218</td>\n      <td>suicide%20bombing</td>\n      <td>Principality of Zeron</td>\n      <td>@RayquazaErk There are Christian terrorists to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2328</th>\n      <td>3346</td>\n      <td>demolished</td>\n      <td></td>\n      <td>@JackMulholland1 I think also became THE MARQU...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4645</th>\n      <td>6604</td>\n      <td>inundated</td>\n      <td></td>\n      <td>Let people surf!  HI is no Waimea bay. It's no...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1773</th>\n      <td>2546</td>\n      <td>collision</td>\n      <td>Denver, Colorado</td>\n      <td>Motorcyclist bicyclist injured in Denver colli...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3872</th>\n      <td>5505</td>\n      <td>flames</td>\n      <td>Somewhere Around You</td>\n      <td>Maryland mansion fire that killed 6 caused by ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2299</th>\n      <td>3299</td>\n      <td>demolish</td>\n      <td>NYHC</td>\n      <td>If you think going to demolish Drake's house o...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1209</th>\n      <td>1741</td>\n      <td>buildings%20burning</td>\n      <td>GO BLUE! HAIL YES!!</td>\n      <td>@1acd4900c1424d1 @FoxNews no one is rioting bu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4054</th>\n      <td>5759</td>\n      <td>forest%20fires</td>\n      <td>Nicola Valley</td>\n      <td>Forest fires &amp;amp; dying salmon: time 2 act no...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7134</th>\n      <td>10218</td>\n      <td>volcano</td>\n      <td></td>\n      <td>http://t.co/Ns1AgGFNxz #shoes Asics GT-II Supe...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5901</th>\n      <td>8429</td>\n      <td>sandstorm</td>\n      <td></td>\n      <td>Watch This Airport Get Swallowed Up By A Sands...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1251</th>\n      <td>1807</td>\n      <td>buildings%20on%20fire</td>\n      <td>UK</td>\n      <td>#TweetLikeItsSeptember11th2001 Those two build...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5243</th>\n      <td>7497</td>\n      <td>oil%20spill</td>\n      <td>New York, NY</td>\n      <td>California oil spill might be larger than proj...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4047</th>\n      <td>5748</td>\n      <td>forest%20fires</td>\n      <td></td>\n      <td>Cartoon Bears. Without them we would qave no k...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2881</th>\n      <td>4141</td>\n      <td>drought</td>\n      <td>miami</td>\n      <td>@_gaabyx we got purple activist I thought it w...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4581</th>\n      <td>6514</td>\n      <td>injuries</td>\n      <td>Madison, WI &amp; St. Louis MO</td>\n      <td>@BuffoonMike I knew mo not doing much would bi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4686</th>\n      <td>6662</td>\n      <td>landslide</td>\n      <td>Austin, Texas</td>\n      <td>@toddstarnes Enjoy the impending landslide Tod...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>292</th>\n      <td>427</td>\n      <td>apocalypse</td>\n      <td>Oregon, USA</td>\n      <td>GO LOOK AT GRIZZLY PEAK RIGHT NOW... It looks ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2698</th>\n      <td>3870</td>\n      <td>detonation</td>\n      <td></td>\n      <td>Ignition Knock (Detonation) Sensor-Senso Stand...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3834</th>\n      <td>5455</td>\n      <td>first%20responders</td>\n      <td></td>\n      <td>This week first responders and DART members ar...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5003</th>\n      <td>7136</td>\n      <td>military</td>\n      <td></td>\n      <td>Lot of 20 Tom Clancy Military Mystery Novels -...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2963</th>\n      <td>4256</td>\n      <td>drowning</td>\n      <td>Coventry</td>\n      <td>Why are you drowning in low self-image? Take t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>396</td>\n      <td>apocalypse</td>\n      <td>ColoRADo</td>\n      <td>I'm gonna fight Taylor as soon as I get there.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2042</th>\n      <td>2932</td>\n      <td>danger</td>\n      <td>Hailing from Dayton</td>\n      <td>I wish I could get Victoria's Secret on front....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3937</th>\n      <td>5598</td>\n      <td>flood</td>\n      <td>New York</td>\n      <td>Spot Flood Combo 53inch 300W Curved Cree LED W...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7590</th>\n      <td>10844</td>\n      <td></td>\n      <td></td>\n      <td>SEVERE WEATHER BULLETIN No. 5 FOR: TYPHOON ÛÏ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2691</th>\n      <td>3860</td>\n      <td>detonation</td>\n      <td></td>\n      <td>Ignition Knock (Detonation) Sensor-Senso fits ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3473</th>\n      <td>4968</td>\n      <td>explosion</td>\n      <td>London, United Kingdom</td>\n      <td>Around 10 injured in explosion in chemical par...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1086</th>\n      <td>1570</td>\n      <td>bomb</td>\n      <td></td>\n      <td>Soul food sound so bomb right now '</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_mis = train.fillna('')\n",
    "train_no_mis.head(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "         id             keyword                    location  \\\n2644   3796         destruction                               \n2227   3185              deluge                               \n5448   7769              police                          UK   \n132     191          aftershock                               \n6845   9810              trauma       Montgomery County, MD   \n5559   7934           rainstorm                               \n1765   2538           collision                               \n1817   2611             crashed                               \n6810   9756             tragedy             Los Angeles, CA   \n4398   6254           hijacking               Athens,Greece   \n1807   2597               crash          In my own world!!!   \n6101   8711             sinking                               \n5701   8136             rescued           Pennsylvania, USA   \n6029   8618             seismic                               \n3012   4326        dust%20storm                 CA via Brum   \n3105   4456        electrocuted                    New York   \n4019   5710              floods                               \n3664   5218            fatality                          NY   \n2164   3105              debris                               \n5406   7717           panicking                               \n3044   4368          earthquake        Melbourne, Australia   \n2534   3638          desolation                               \n1507   2175        catastrophic                               \n4669   6636          inundation                               \n472     682              attack            portland, oregon   \n1174   1691   bridge%20collapse               Pittsburgh PA   \n7345  10517            wildfire                               \n5673   8096             rescued                     Nigeria   \n6582   9421           survivors                   Sao Paulo   \n5756   8216                riot                               \n1511   2181        catastrophic                    New York   \n4717   6707                lava             Santa Maria, CA   \n5065   7221  natural%20disaster                  Aurora, IL   \n3624   5172          fatalities           Chamblee, Georgia   \n5265   7527         oil%20spill                Kamloops, BC   \n6372   9106      suicide%20bomb                 dorito land   \n5324   7602         pandemonium           Dallas Fort-Worth   \n263     380        annihilation                 Phoenix, AZ   \n3082   4424         electrocute                               \n3753   5332                fire                St.Cloud, MN   \n6699   9596             thunder                   IndiLand    \n2418   3477            derailed                          US   \n5481   7822          quarantine          all over the world   \n6539   9351            survived         Mumbai, Maharashtra   \n23       34                                                   \n4518   6420           hurricane                               \n3315   4749            evacuate                       U.S.A   \n4359   6193            hijacker                Sarasota, FL   \n3309   4740            evacuate  Gold Coast, Qld, Australia   \n7253  10383             weapons             The Netherlands   \n\n                                                   text  target  \n2644  So you have a new weapon that can cause un-ima...       1  \n2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n132   Aftershock back to school kick off was great. ...       0  \n6845  in response to trauma Children of Addicts deve...       0  \n5559  @Calum5SOS you look like you got caught in a r...       0  \n1765  my favorite lady came to our volunteer meeting...       1  \n1817  @brianroemmele UX fail of EMV - people want to...       1  \n6810  Can't find my ariana grande shirt  this is a f...       0  \n4398  The Murderous Story Of AmericaÛªs First Hijac...       1  \n1807  AKILAH WORLD NEWS Cop pulls man from car to av...       1  \n6101                We walk the plank of a sinking ship       0  \n5701  @Zak_Bagans pets r like part of the family. I ...       0  \n6029  ON THE USE OF PERFORATED METAL SHEAR PANEL SFO...       0  \n3012  The answer my friend is yelling in the wind-my...       0  \n3105  Woman electrocuted #Red #Redblood #videoclip h...       0  \n4019  Who is bringing the tornadoes and floods. Who ...       0  \n3664  So these savages leaked Thomas Brady gangsterm...       0  \n2164  Malaysia Airlines Flight 370 that Disappeared ...       1  \n5406  People are finally panicking about cable TV ht...       0  \n3044  Nepal earthquake 3 months on: Women fear abuse...       1  \n2534  Escape The Heat (and the #ORShow) for a trail ...       0  \n1507  @marginoferror I wish going custom ROM weren't...       0  \n4669  @ZachLowe_NBA there are a few reasons for that...       0  \n472   illegal alien released by Obama/DHS 4 times Ch...       1  \n1174  @BloopAndABlast Because I need to know if I'm ...       0  \n7345  ! Residents Return To Destroyed Homes As Washi...       1  \n5673  4 kidnapped ladies rescued by police in Enugu ...       1  \n6582  Hiroshima survivors fight nuclear industry in ...       1  \n5756  Stuart Broad Takes Eight Before Joe Root Runs ...       0  \n1511  @MyVintageSoul ...of the British upper class a...       0  \n4717  Neighbor kids stopped to watch me play Disney'...       0  \n5065  I added a video to a @YouTube playlist http://...       0  \n3624  As of the 6-month mark there were a total of 6...       1  \n5265  @Kinder_Morgan can'twon't tell @cityofkamloops...       1  \n6372                               she's a suicide bomb       0  \n5324  Pandemonium In Aba As Woman Delivers Baby With...       1  \n263   Hey #AZ: Sign this petition to save the #WildH...       0  \n3082  Why does my phone electrocute me when it's cha...       0  \n3753  Dear @CanonUSAimaging I brought it ;) #CanonBr...       0  \n6699                 My mama scared of the thunder ????       0  \n2418  Breakfast links: Work from home: Derailed: An ...       1  \n5481  #wired #business Reddit Will Now Quarantine Of...       0  \n6539  It's a miracle that mankind survived 70 years ...       1  \n23                                What a wonderful day!       0  \n4518  @pattonoswalt @FoxNews Wait I thought Fecal Hu...       1  \n3315  California wildfires force thousands to evacua...       1  \n4359  Remove the http://t.co/Xxj2B4JxRt and Linkury ...       0  \n3309  myGC: Broken powerlines evacuate Gold Coast tr...       1  \n7253  'The Reagan Administration had arranged for Is...       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2644</th>\n      <td>3796</td>\n      <td>destruction</td>\n      <td></td>\n      <td>So you have a new weapon that can cause un-ima...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2227</th>\n      <td>3185</td>\n      <td>deluge</td>\n      <td></td>\n      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5448</th>\n      <td>7769</td>\n      <td>police</td>\n      <td>UK</td>\n      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>191</td>\n      <td>aftershock</td>\n      <td></td>\n      <td>Aftershock back to school kick off was great. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6845</th>\n      <td>9810</td>\n      <td>trauma</td>\n      <td>Montgomery County, MD</td>\n      <td>in response to trauma Children of Addicts deve...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5559</th>\n      <td>7934</td>\n      <td>rainstorm</td>\n      <td></td>\n      <td>@Calum5SOS you look like you got caught in a r...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1765</th>\n      <td>2538</td>\n      <td>collision</td>\n      <td></td>\n      <td>my favorite lady came to our volunteer meeting...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1817</th>\n      <td>2611</td>\n      <td>crashed</td>\n      <td></td>\n      <td>@brianroemmele UX fail of EMV - people want to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6810</th>\n      <td>9756</td>\n      <td>tragedy</td>\n      <td>Los Angeles, CA</td>\n      <td>Can't find my ariana grande shirt  this is a f...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4398</th>\n      <td>6254</td>\n      <td>hijacking</td>\n      <td>Athens,Greece</td>\n      <td>The Murderous Story Of AmericaÛªs First Hijac...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1807</th>\n      <td>2597</td>\n      <td>crash</td>\n      <td>In my own world!!!</td>\n      <td>AKILAH WORLD NEWS Cop pulls man from car to av...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6101</th>\n      <td>8711</td>\n      <td>sinking</td>\n      <td></td>\n      <td>We walk the plank of a sinking ship</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5701</th>\n      <td>8136</td>\n      <td>rescued</td>\n      <td>Pennsylvania, USA</td>\n      <td>@Zak_Bagans pets r like part of the family. I ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6029</th>\n      <td>8618</td>\n      <td>seismic</td>\n      <td></td>\n      <td>ON THE USE OF PERFORATED METAL SHEAR PANEL SFO...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3012</th>\n      <td>4326</td>\n      <td>dust%20storm</td>\n      <td>CA via Brum</td>\n      <td>The answer my friend is yelling in the wind-my...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3105</th>\n      <td>4456</td>\n      <td>electrocuted</td>\n      <td>New York</td>\n      <td>Woman electrocuted #Red #Redblood #videoclip h...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4019</th>\n      <td>5710</td>\n      <td>floods</td>\n      <td></td>\n      <td>Who is bringing the tornadoes and floods. Who ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3664</th>\n      <td>5218</td>\n      <td>fatality</td>\n      <td>NY</td>\n      <td>So these savages leaked Thomas Brady gangsterm...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2164</th>\n      <td>3105</td>\n      <td>debris</td>\n      <td></td>\n      <td>Malaysia Airlines Flight 370 that Disappeared ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5406</th>\n      <td>7717</td>\n      <td>panicking</td>\n      <td></td>\n      <td>People are finally panicking about cable TV ht...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3044</th>\n      <td>4368</td>\n      <td>earthquake</td>\n      <td>Melbourne, Australia</td>\n      <td>Nepal earthquake 3 months on: Women fear abuse...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2534</th>\n      <td>3638</td>\n      <td>desolation</td>\n      <td></td>\n      <td>Escape The Heat (and the #ORShow) for a trail ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1507</th>\n      <td>2175</td>\n      <td>catastrophic</td>\n      <td></td>\n      <td>@marginoferror I wish going custom ROM weren't...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4669</th>\n      <td>6636</td>\n      <td>inundation</td>\n      <td></td>\n      <td>@ZachLowe_NBA there are a few reasons for that...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>472</th>\n      <td>682</td>\n      <td>attack</td>\n      <td>portland, oregon</td>\n      <td>illegal alien released by Obama/DHS 4 times Ch...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1174</th>\n      <td>1691</td>\n      <td>bridge%20collapse</td>\n      <td>Pittsburgh PA</td>\n      <td>@BloopAndABlast Because I need to know if I'm ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7345</th>\n      <td>10517</td>\n      <td>wildfire</td>\n      <td></td>\n      <td>! Residents Return To Destroyed Homes As Washi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5673</th>\n      <td>8096</td>\n      <td>rescued</td>\n      <td>Nigeria</td>\n      <td>4 kidnapped ladies rescued by police in Enugu ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6582</th>\n      <td>9421</td>\n      <td>survivors</td>\n      <td>Sao Paulo</td>\n      <td>Hiroshima survivors fight nuclear industry in ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5756</th>\n      <td>8216</td>\n      <td>riot</td>\n      <td></td>\n      <td>Stuart Broad Takes Eight Before Joe Root Runs ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1511</th>\n      <td>2181</td>\n      <td>catastrophic</td>\n      <td>New York</td>\n      <td>@MyVintageSoul ...of the British upper class a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4717</th>\n      <td>6707</td>\n      <td>lava</td>\n      <td>Santa Maria, CA</td>\n      <td>Neighbor kids stopped to watch me play Disney'...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5065</th>\n      <td>7221</td>\n      <td>natural%20disaster</td>\n      <td>Aurora, IL</td>\n      <td>I added a video to a @YouTube playlist http://...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3624</th>\n      <td>5172</td>\n      <td>fatalities</td>\n      <td>Chamblee, Georgia</td>\n      <td>As of the 6-month mark there were a total of 6...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5265</th>\n      <td>7527</td>\n      <td>oil%20spill</td>\n      <td>Kamloops, BC</td>\n      <td>@Kinder_Morgan can'twon't tell @cityofkamloops...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6372</th>\n      <td>9106</td>\n      <td>suicide%20bomb</td>\n      <td>dorito land</td>\n      <td>she's a suicide bomb</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5324</th>\n      <td>7602</td>\n      <td>pandemonium</td>\n      <td>Dallas Fort-Worth</td>\n      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>263</th>\n      <td>380</td>\n      <td>annihilation</td>\n      <td>Phoenix, AZ</td>\n      <td>Hey #AZ: Sign this petition to save the #WildH...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3082</th>\n      <td>4424</td>\n      <td>electrocute</td>\n      <td></td>\n      <td>Why does my phone electrocute me when it's cha...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3753</th>\n      <td>5332</td>\n      <td>fire</td>\n      <td>St.Cloud, MN</td>\n      <td>Dear @CanonUSAimaging I brought it ;) #CanonBr...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6699</th>\n      <td>9596</td>\n      <td>thunder</td>\n      <td>IndiLand</td>\n      <td>My mama scared of the thunder ????</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2418</th>\n      <td>3477</td>\n      <td>derailed</td>\n      <td>US</td>\n      <td>Breakfast links: Work from home: Derailed: An ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5481</th>\n      <td>7822</td>\n      <td>quarantine</td>\n      <td>all over the world</td>\n      <td>#wired #business Reddit Will Now Quarantine Of...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6539</th>\n      <td>9351</td>\n      <td>survived</td>\n      <td>Mumbai, Maharashtra</td>\n      <td>It's a miracle that mankind survived 70 years ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>34</td>\n      <td></td>\n      <td></td>\n      <td>What a wonderful day!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4518</th>\n      <td>6420</td>\n      <td>hurricane</td>\n      <td></td>\n      <td>@pattonoswalt @FoxNews Wait I thought Fecal Hu...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3315</th>\n      <td>4749</td>\n      <td>evacuate</td>\n      <td>U.S.A</td>\n      <td>California wildfires force thousands to evacua...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4359</th>\n      <td>6193</td>\n      <td>hijacker</td>\n      <td>Sarasota, FL</td>\n      <td>Remove the http://t.co/Xxj2B4JxRt and Linkury ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3309</th>\n      <td>4740</td>\n      <td>evacuate</td>\n      <td>Gold Coast, Qld, Australia</td>\n      <td>myGC: Broken powerlines evacuate Gold Coast tr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7253</th>\n      <td>10383</td>\n      <td>weapons</td>\n      <td>The Netherlands</td>\n      <td>'The Reagan Administration had arranged for Is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_no_mis = test.fillna('')\n",
    "test_no_mis.head(50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Задание 2 (1 балл)\n",
    "Давайте немного посмотрим на наши данные. Визуализируйте (где явно просят) или выведете информацию о следующем:\n",
    "\n",
    "1. Какое распределение классов в обучающей выборке?\n",
    "2. Посмотрите на колонку \"keyword\" - возьмите 10 наиболее встречающихся значений, постройте ступенчатую диаграмму распределения классов в зависимости от значения keyword, сделайте выводы."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "target\n0         3024\n1         2305\ndtype: int64"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_mis.value_counts(['target'])\n",
    "# Классы слегка несбалансированны."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "keyword              \n                         44\ndamage                   36\nsiren                    35\nwreckage                 34\nfatalities               33\n                         ..\nforest%20fire            16\nepicentre                10\ninundation                7\nradiation%20emergency     7\nthreat                    6\nLength: 222, dtype: int64"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_mis.value_counts(['keyword'], sort=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1080x576 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "top_keywords = set(train_no_mis.value_counts(['keyword'], sort=True)[:10].index.get_level_values(level=0))\n",
    "\n",
    "top_indexes = []\n",
    "for i, row in train_no_mis.iterrows():\n",
    "    if row['keyword'] in top_keywords:\n",
    "        top_indexes.append(i)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "sns.histplot(\n",
    "    train_no_mis[train_no_mis.index.isin(top_indexes)],\n",
    "    x='keyword', hue='target',\n",
    "    multiple='stack',\n",
    "    palette=['orange', 'g']\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "# Число фейковых твитов слегка превосходит число твитов с реальными катастрофами."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3 (0.5 балла) \n",
    "\n",
    "В этом задании предлагается объединить все три текстовых столбца в один (просто сконкатенировать cтроки) и убрать столбец с индексом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "      target                                           combined\n1186       0  bridge%20collapse  Ashes 2015: AustraliaÛªs c...\n4071       1  hail Carol Stream, Illinois GREAT MICHIGAN TEC...\n5461       1  police Houston  CNN: Tennessee movie theater s...\n5787       1  rioting  Still rioting in a couple of hours le...\n7445       0  wounds Lake Highlands Crack in the path where ...\n...      ...                                                ...\n5226       0  obliteration Merica! @Eganator2000 There aren'...\n5390       0  panic  just had a panic attack bc I don't have...\n860        0  blood  Omron HEM-712C Automatic Blood Pressure...\n7603       1    Officials say a quarantine is in place at an...\n7270       1  whirlwind Stamford & Cork (& Shropshire) I mov...\n\n[5329 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>combined</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1186</th>\n      <td>0</td>\n      <td>bridge%20collapse  Ashes 2015: AustraliaÛªs c...</td>\n    </tr>\n    <tr>\n      <th>4071</th>\n      <td>1</td>\n      <td>hail Carol Stream, Illinois GREAT MICHIGAN TEC...</td>\n    </tr>\n    <tr>\n      <th>5461</th>\n      <td>1</td>\n      <td>police Houston  CNN: Tennessee movie theater s...</td>\n    </tr>\n    <tr>\n      <th>5787</th>\n      <td>1</td>\n      <td>rioting  Still rioting in a couple of hours le...</td>\n    </tr>\n    <tr>\n      <th>7445</th>\n      <td>0</td>\n      <td>wounds Lake Highlands Crack in the path where ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5226</th>\n      <td>0</td>\n      <td>obliteration Merica! @Eganator2000 There aren'...</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>0</td>\n      <td>panic  just had a panic attack bc I don't have...</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>0</td>\n      <td>blood  Omron HEM-712C Automatic Blood Pressure...</td>\n    </tr>\n    <tr>\n      <th>7603</th>\n      <td>1</td>\n      <td>Officials say a quarantine is in place at an...</td>\n    </tr>\n    <tr>\n      <th>7270</th>\n      <td>1</td>\n      <td>whirlwind Stamford &amp; Cork (&amp; Shropshire) I mov...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5329 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_merge = ['keyword', 'location', 'text']\n",
    "to_drop = to_merge.copy()\n",
    "to_drop.append('id')\n",
    "\n",
    "train_no_mis['combined'] = train_no_mis[to_merge].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "test_no_mis['combined'] = test_no_mis[to_merge].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "train_combined = train_no_mis.drop(to_drop, axis=1)\n",
    "test_combined = test_no_mis.drop(to_drop, axis=1)\n",
    "train_combined"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4 (0.5 балла)\n",
    "\n",
    "Далее мы будем пока работать только с train частью.\n",
    "\n",
    "1. Предобработайте данные (train часть) с помощью CountVectorizer.\n",
    "2. Какого размера получилась матрица?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "train_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "train_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(5329, 18455)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5 (1 балл)\n",
    "\n",
    "В предыдущем пункте у вас должна была получиться достаточно большая матрица.\n",
    "Если вы взгляните на текст, то увидете, что там есть множество специальных символов, ссылок и прочего мусора.\n",
    "\n",
    "Давайте также посмотрим на словарь, который получился в результате построения CountVectorizer, его можно найти в поле vocabulary_ инстанса этого класса. Давайте напишем функцию, которая печает ответы на следующие вопросы:\n",
    "\n",
    "1. Найдите в этом словаре все слова, которые содержат цифры. Сколько таких слов нашлось?\n",
    "\n",
    "2. Найдите все слова, которые содержат символы пунктуации. Сколько таких слов нашлось? \n",
    "\n",
    "3. Сколько хэштегов (токен начинается на #) и упоминаний (токен начинается на @) осталось в словаре?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenized_info(voc):\n",
    "    has_digits = list()\n",
    "    has_punct = list()\n",
    "    hash_ate = list()\n",
    "\n",
    "    for word in voc.keys():\n",
    "        if re.search('\\d+', word) is not None:\n",
    "            has_digits.append(word)\n",
    "\n",
    "        if any([letter in string.punctuation for letter in word]):\n",
    "            has_punct.append(word)\n",
    "\n",
    "        if word[0] == '@' or word[0] == '#':\n",
    "            hash_ate.append(word)\n",
    "\n",
    "    print('Number of words with digits inside:', len(has_digits))\n",
    "    print('Number of words with punctuation inside:', len(has_punct))\n",
    "    print('Number of words starts with # or @:', len(hash_ate))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with digits inside: 3812\n",
      "Number of words with punctuation inside: 315\n",
      "Number of words starts with # or @: 0\n"
     ]
    }
   ],
   "source": [
    "tokenized_info(vectorizer.vocabulary_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6 (0.5 балла)\n",
    "\n",
    "Вспомним, что на семинаре по текстам мы узнали, что в nltk есть специальный токенизатор для текстов - TweetTokenizer. Попробуем применить CountVectorizer с этим токенизатором. Ответьте на все вопросы из предыдущего пункта для TweetTokenizer и сравните результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TweetTokenizer in module nltk.tokenize.casual:\n",
      "\n",
      "class TweetTokenizer(nltk.tokenize.api.TokenizerI)\n",
      " |  TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False, match_phone_numbers=True)\n",
      " |  \n",
      " |  Tokenizer for tweets.\n",
      " |  \n",
      " |      >>> from nltk.tokenize import TweetTokenizer\n",
      " |      >>> tknzr = TweetTokenizer()\n",
      " |      >>> s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
      " |      >>> tknzr.tokenize(s0)\n",
      " |      ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3'\n",
      " |      , 'and', 'some', 'arrows', '<', '>', '->', '<--']\n",
      " |  \n",
      " |  Examples using `strip_handles` and `reduce_len parameters`:\n",
      " |  \n",
      " |      >>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
      " |      >>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
      " |      >>> tknzr.tokenize(s1)\n",
      " |      [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TweetTokenizer\n",
      " |      nltk.tokenize.api.TokenizerI\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, preserve_case=True, reduce_len=False, strip_handles=False, match_phone_numbers=True)\n",
      " |      Create a `TweetTokenizer` instance with settings for use in the `tokenize` method.\n",
      " |      \n",
      " |      :param preserve_case: Flag indicating whether to preserve the casing (capitalisation)\n",
      " |          of text used in the `tokenize` method. Defaults to True.\n",
      " |      :type preserve_case: bool\n",
      " |      :param reduce_len: Flag indicating whether to replace repeated character sequences\n",
      " |          of length 3 or greater with sequences of length 3. Defaults to False.\n",
      " |      :type reduce_len: bool\n",
      " |      :param strip_handles: Flag indicating whether to remove Twitter handles of text used\n",
      " |          in the `tokenize` method. Defaults to False.\n",
      " |      :type strip_handles: bool\n",
      " |      :param match_phone_numbers: Flag indicating whether the `tokenize` method should look\n",
      " |          for phone numbers. Defaults to True.\n",
      " |      :type match_phone_numbers: bool\n",
      " |  \n",
      " |  tokenize(self, text: str) -> List[str]\n",
      " |      Tokenize the input text.\n",
      " |      \n",
      " |      :param text: str\n",
      " |      :rtype: list(str)\n",
      " |      :return: a tokenized list of strings; joining this list returns        the original string if `preserve_case=False`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  PHONE_WORD_RE\n",
      " |      Secondary core TweetTokenizer regex\n",
      " |  \n",
      " |  WORD_RE\n",
      " |      Core TweetTokenizer regex\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  span_tokenize(self, s: str) -> Iterator[Tuple[int, int]]\n",
      " |      Identify the tokens using integer offsets ``(start_i, end_i)``,\n",
      " |      where ``s[start_i:end_i]`` is the corresponding token.\n",
      " |      \n",
      " |      :rtype: Iterator[Tuple[int, int]]\n",
      " |  \n",
      " |  span_tokenize_sents(self, strings: List[str]) -> Iterator[List[Tuple[int, int]]]\n",
      " |      Apply ``self.span_tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.span_tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :yield: List[Tuple[int, int]]\n",
      " |  \n",
      " |  tokenize_sents(self, strings: List[str]) -> List[List[str]]\n",
      " |      Apply ``self.tokenize()`` to each element of ``strings``.  I.e.:\n",
      " |      \n",
      " |          return [self.tokenize(s) for s in strings]\n",
      " |      \n",
      " |      :rtype: List[List[str]]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tokenize.api.TokenizerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Чтобы узнать, какие параметры есть у этого токенайзера - используйте help(TweetTokenizer)\n",
    "# Для того, чтобы передать токенайзер в CountVectorizer используйте параметр tokenizer\n",
    "help(TweetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz = TweetTokenizer()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tknz.tokenize)\n",
    "train_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "train_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words with digits inside: 3939\n",
      "Number of words with punctuation inside: 7338\n",
      "Number of words starts with # or @: 3149\n"
     ]
    }
   ],
   "source": [
    "tokenized_info(vectorizer.vocabulary_)\n",
    "# Данный токенайзер не разбивает слова при встрече твиттерных символов, что в нашем случае очень полезно.\n",
    "# Кроме того, он пропускает множество слов с пунктуацией внутри."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7 (2 балла)\n",
    "\n",
    "В scikit-learn мы можем оценивать процесс подсчета матрицы через CountVectorizer. У CountVectorizer, как и у других наследников \\_VectorizerMixin, есть аргумент tokenizer и preprocessor. preprocessor применится в самом начале к каждой строке вашего датасета, tokenizer же должен принять строку и вернуть токены.\n",
    "Давайте напишем кастомный токенайзер, которые сделает все, что нам нужно: \n",
    "\n",
    "0. Приведет все буквы к нижнему регистру\n",
    "1. Разобьет текст на токены с помощью TweetTokenizer из пакета nltk\n",
    "2. Удалит все токены содержащие не латинские буквы, кроме смайликов (будем считать ими токены содержащие только пунктуацию и, как минимум, одну скобочку) и хэштегов, которые после начальной # содержат только латинские буквы.\n",
    "3. Удалит все токены, которые перечислены в nltk.corpus.stopwords.words('english')\n",
    "4. Проведет стемминг с помощью SnowballStemmer\n",
    "\n",
    "Продемонстрируйте работу вашей функции на первых десяти текстах в обучающей выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dm1tr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from typing import List\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "class SupaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.__personal_tweet_tokenizer = TweetTokenizer()\n",
    "        self.__personal_snowball = SnowballStemmer('english')\n",
    "\n",
    "    def __is_smile(self, word):\n",
    "        smiling_syms = {'{', '}', '[', ']', '(', ')'}\n",
    "        has_smile = False\n",
    "        punct_only = True\n",
    "        for letter in word:\n",
    "            if letter not in string.punctuation:\n",
    "                punct_only = False\n",
    "            if letter in smiling_syms:\n",
    "                has_smile = True\n",
    "\n",
    "        return has_smile and punct_only\n",
    "\n",
    "    def __is_hashtag_or_latin_only(self, word):\n",
    "        if word[0] == '#':\n",
    "            word = word[1:]\n",
    "\n",
    "        return all([letter in string.ascii_letters for letter in word])\n",
    "\n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize the input text.\n",
    "\n",
    "        :param text: str\n",
    "        :rtype: list(str)\n",
    "        :return: a tokenized list of strings; joining this list returns\\\n",
    "        the original string if `preserve_case=False`.\n",
    "        \"\"\"\n",
    "\n",
    "        text_twtokenized = self.__personal_tweet_tokenizer.tokenize(text.lower())\n",
    "        clear_text = list()\n",
    "\n",
    "        for word in text_twtokenized:\n",
    "            if self.__is_smile(word) or self.__is_hashtag_or_latin_only(\n",
    "                    word) and word not in nltk.corpus.stopwords.words('english'):\n",
    "                clear_text.append(self.__personal_snowball.stem(word))\n",
    "\n",
    "        return clear_text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bridg', 'ash', 'australia', 'collaps', 'trent', 'bridg', 'among', 'worst', 'histori', 'england', 'bundl', 'australia']\n",
      "['hail', 'carol', 'stream', 'illinoi', 'great', 'michigan', 'techniqu', 'camp', 'thank', '#goblu', '#wrestleon']\n",
      "['polic', 'houston', 'cnn', 'tennesse', 'movi', 'theater', 'shoot', 'suspect', 'kill', 'polic']\n",
      "['riot', 'still', 'riot', 'coupl', 'hour', 'left', 'class']\n",
      "['wound', 'lake', 'highland', 'crack', 'path', 'wipe', 'morn', 'beach', 'run', 'surfac', 'wound', 'left', 'elbow', 'right', 'knee']\n",
      "['airplan', 'somewher', 'expert', 'franc', 'begin', 'examin', 'airplan', 'debri', 'found', 'reunion', 'island', 'french', 'air', 'accid', 'expert', '#mlb']\n",
      "['bloodi', 'isol', 'citi', 'world', 'perth', 'came', 'kill', 'indian', 'fun', 'video', 'smirk', 'remorseless', 'pakistani', 'killer', 'show', 'boast']\n",
      "['burn', 'except', 'idk', 'realli', 'burn']\n",
      "['destroy', '(', 'ask', ')', 'destroy', 'hous']\n",
      "['wound', 'maracay', 'nirgua', 'venezuela', 'polic', 'offic', 'wound', 'suspect', 'dead', 'exchang', 'shot']\n"
     ]
    }
   ],
   "source": [
    "supa_tokenizer = SupaTokenizer()\n",
    "for i in range(10):\n",
    "    print(supa_tokenizer.tokenize(train_combined.iloc[i]['combined']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 (1 балл)\n",
    "\n",
    "1. Примените CountVectorizer с реализованным выше токенизатором к обучающим и тестовым выборкам.\n",
    "2. Обучите LogisticRegression на полученных признаках.\n",
    "3. Посчитайте метрику f1-score на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknz = SupaTokenizer()\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tknz.tokenize)\n",
    "train_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "train_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "test_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_tokenized.toarray(), train_combined['target'])\n",
    "y_pred = clf.predict(test_tokenized.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.752017213555675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1318\n",
      "           1       0.78      0.72      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "print()\n",
    "print(classification_report(test_combined['target'], y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 (1 балл)\n",
    "\n",
    "1. Повторите 7 задание, но с tf-idf векторизатором. Как изменилось качество?\n",
    "2. Мы можем еще сильнее уменьшить размер нашей матрицы, если отбросим значения df близкие к единице. Скорее всего такие слова не несут много информации о категории, так как встречаются достаточно часто. Ограничьте максимальный df в параметрах TfIdfVectorizer, поставьте верхнюю границу равную 0.9. Как изменился размер матрицы, как изменилось качество?\n",
    "3. Также мы можем уменьшить размер матрицы, удаляя слова со слишком маленьким df. Удалось ли добиться улучшения качества? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tknz = SupaTokenizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer=tknz.tokenize, max_df=0.9)\n",
    "\n",
    "train_tfidf_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "train_tfidf_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "test_tfidf_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_tfidf_tokenized.toarray(), train_combined['target'])\n",
    "y_pred = clf.predict(test_tfidf_tokenized.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.7412353923205343\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1318\n",
      "           1       0.80      0.69      0.74       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.78      0.79      2284\n",
      "weighted avg       0.80      0.80      0.79      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "print()\n",
    "print(classification_report(test_combined['target'], y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf size (5329, 10486)\n",
      "Count size (5329, 10486)\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf size', train_tfidf_tokenized.shape)\n",
    "print('Count size', train_tokenized.shape)\n",
    "# Видно, что ничего не поменялось, значит нет слов, которые так часто встречаются.\n",
    "# Опытным путем было выяснено, что max_df начинал давать эффект в районе 0.04."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.7463848720800891\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84      1318\n",
      "           1       0.81      0.69      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n",
      "(5329, 4527)\n"
     ]
    }
   ],
   "source": [
    "tknz = SupaTokenizer()\n",
    "vectorizer = TfidfVectorizer(tokenizer=tknz.tokenize, max_df=0.9, min_df=0.0002)\n",
    "\n",
    "train_tfidf_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "test_tfidf_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_tfidf_tokenized.toarray(), train_combined['target'])\n",
    "y_pred = clf.predict(test_tfidf_tokenized.toarray())\n",
    "\n",
    "print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "print()\n",
    "print(classification_report(test_combined['target'], y_pred))\n",
    "print(train_tfidf_tokenized.shape)\n",
    "\n",
    "# Слегка подняли f-меру путем указания min_df."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 10 (1 балл)\n",
    "\n",
    "Еще один популяпный трюк, который позволит уменьшить количество признаков называется hashing trick. Его суть в том, то мы случайно группируем признаки ииии  ..... складываем их! А потом удаляем исходные признаки. В итоге все наши признаки это просто суммы исходных. Звучит странно, но это отлично работает. Давайте проверим этот трюк в нашем сеттинге.\n",
    "Также при таком подходе вам не нужно хранить словарь token->index, что тоже иногда полезно.\n",
    "\n",
    "1. Повторите задание 7 с HashingVectorizer, укажите количество фичей равное 5000.\n",
    "2. Какой из подходов показал самый высокий результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dm1tr\\desktop\\ida-ml\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.30151134],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       ...,\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ],\n       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n        0.        ]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "tknz = SupaTokenizer()\n",
    "vectorizer = HashingVectorizer(tokenizer=supa_tokenizer.tokenize, n_features=5000)\n",
    "\n",
    "train_hash_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "train_hash_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hash_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "test_hash_tokenized.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(train_hash_tokenized.toarray(), train_combined['target'])\n",
    "y_pred = clf.predict(test_hash_tokenized.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.7190635451505016\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82      1318\n",
      "           1       0.78      0.67      0.72       966\n",
      "\n",
      "    accuracy                           0.78      2284\n",
      "   macro avg       0.78      0.76      0.77      2284\n",
      "weighted avg       0.78      0.78      0.78      2284\n",
      "\n",
      "(5329, 5000)\n"
     ]
    }
   ],
   "source": [
    "print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "print()\n",
    "print(classification_report(test_combined['target'], y_pred))\n",
    "print(train_hash_tokenized.shape)\n",
    "\n",
    "# Результат хуже двух предыдущих ((\n",
    "# В целом Count и Tfidf показывают примерно одинаковый (кто-то где-то лучше, кто-то где-то хуже) результат."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 11 (1 балл)\n",
    "\n",
    "В этом задании нужно добиться f1 меры хотя в 0.75 на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.752017213555675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1318\n",
      "           1       0.78      0.72      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tknz.tokenize)\n",
    "\n",
    "train_count_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "test_count_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_count_tokenized.toarray(), train_combined['target'])\n",
    "y_pred = clf.predict(test_count_tokenized.toarray())\n",
    "\n",
    "print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "print()\n",
    "print(classification_report(test_combined['target'], y_pred))\n",
    "\n",
    "# Тут уже f-мера 0.75.\n",
    "# Попробуем поиграться с n-граммами."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.752017213555675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1318\n",
      "           1       0.78      0.72      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n",
      "F1 score on test: 0.7505470459518598\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83      1318\n",
      "           1       0.80      0.71      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n",
      "F1 score on test: 0.745969983324069\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.83      1318\n",
      "           1       0.81      0.69      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n",
      "F1 score on test: 0.5475339528234453\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.96      0.80      1318\n",
      "           1       0.88      0.40      0.55       966\n",
      "\n",
      "    accuracy                           0.72      2284\n",
      "   macro avg       0.78      0.68      0.67      2284\n",
      "weighted avg       0.77      0.72      0.69      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "combs_of = [1, 2, 3]\n",
    "combs = list()\n",
    "combs.append((1, 1))\n",
    "combs += itertools.combinations(combs_of, 2)\n",
    "\n",
    "for co in combs:\n",
    "    vectorizer = CountVectorizer(tokenizer=tknz.tokenize, ngram_range=co)\n",
    "\n",
    "    train_count_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "    test_count_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_count_tokenized.toarray(), train_combined['target'])\n",
    "    y_pred = clf.predict(test_count_tokenized.toarray())\n",
    "\n",
    "    print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "    print()\n",
    "    print(classification_report(test_combined['target'], y_pred))\n",
    "\n",
    "# Замечен неплохой отрицательный рост)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.7180823345492444\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.80      1318\n",
      "           1       0.72      0.71      0.72       966\n",
      "\n",
      "    accuracy                           0.76      2284\n",
      "   macro avg       0.76      0.76      0.76      2284\n",
      "weighted avg       0.76      0.76      0.76      2284\n",
      "\n",
      "F1 score on test: 0.7308337996642418\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83      1318\n",
      "           1       0.80      0.68      0.73       966\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.77      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n",
      "F1 score on test: 0.7417943107221007\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.83      1318\n",
      "           1       0.79      0.70      0.74       966\n",
      "\n",
      "    accuracy                           0.79      2284\n",
      "   macro avg       0.79      0.78      0.78      2284\n",
      "weighted avg       0.79      0.79      0.79      2284\n",
      "\n",
      "F1 score on test: 0.618158403090792\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.92      0.80      1318\n",
      "           1       0.82      0.50      0.62       966\n",
      "\n",
      "    accuracy                           0.74      2284\n",
      "   macro avg       0.77      0.71      0.71      2284\n",
      "weighted avg       0.76      0.74      0.73      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "combs_of = [1, 2, 3]\n",
    "combs = list()\n",
    "combs.append((1, 1))\n",
    "combs += itertools.combinations(combs_of, 2)\n",
    "\n",
    "for co in combs:\n",
    "    vectorizer = CountVectorizer(tokenizer=TweetTokenizer().tokenize, ngram_range=co)\n",
    "\n",
    "    train_count_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "    test_count_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(train_count_tokenized.toarray(), train_combined['target'])\n",
    "    y_pred = clf.predict(test_count_tokenized.toarray())\n",
    "\n",
    "    print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "    print()\n",
    "    print(classification_report(test_combined['target'], y_pred))\n",
    "\n",
    "# SVM еще хуже(\n",
    "# Похоже log-loss с униграммами работает лучше всего с его-то f-мерой > 0.75."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.7433333333333335\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83      1318\n",
      "           1       0.80      0.69      0.74       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.78      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "tknz = SupaTokenizer()\n",
    "vectorizer = CountVectorizer(tokenizer=tknz.tokenize)\n",
    "\n",
    "train_count_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "test_count_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "train_count_tokenized_scaled = scaler.fit_transform(train_count_tokenized.toarray())\n",
    "test_count_tokenized_scaled = scaler.transform(test_count_tokenized.toarray())\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_count_tokenized_scaled, train_combined['target'])\n",
    "y_pred = clf.predict(test_count_tokenized_scaled)\n",
    "\n",
    "print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "print()\n",
    "print(classification_report(test_combined['target'], y_pred))\n",
    "\n",
    "# В конце концов масштабириуем. Перебрал скалеры, все равно не нашел тот, что повысит f-меру.\n",
    "# Возможно, комбинация скалеров, токенайзеров, классификаторов, кол-ва n-грамм и даст лучший результат,\n",
    "# но пока что оставим первый вариант."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.752017213555675\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83      1318\n",
      "           1       0.78      0.72      0.75       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.80      0.79      0.79      2284\n",
      "weighted avg       0.80      0.80      0.80      2284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=tknz.tokenize)\n",
    "\n",
    "train_count_tokenized = vectorizer.fit_transform(train_combined['combined'])\n",
    "test_count_tokenized = vectorizer.transform(test_combined['combined'])\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_count_tokenized.toarray(), train_combined['target'])\n",
    "y_pred = clf.predict(test_count_tokenized.toarray())\n",
    "\n",
    "print('F1 score on test:', f1_score(test_combined['target'], y_pred))\n",
    "print()\n",
    "print(classification_report(test_combined['target'], y_pred))\n",
    "\n",
    "# Поэтому оставим этот (скучный -_-) вариант."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}